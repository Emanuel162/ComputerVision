{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "3YXpQBvjaWG8",
    "ZR7ZuNvapJCv",
    "tyoM1_yiYILG"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Computer Vision - Assignment 2"
   ],
   "metadata": {
    "id": "-wTIjlgKFNL_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Part - Spatial Filters"
   ],
   "metadata": {
    "id": "3YXpQBvjaWG8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "from scipy.signal import correlate2d, convolve2d\n",
    "\n",
    "# Misc.\n",
    "def url2img(url):\n",
    "  req = urllib.request.urlopen(url)\n",
    "  arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "  return cv2.imdecode(arr, -1)"
   ],
   "metadata": {
    "id": "PbV3peY1pwiL",
    "ExecuteTime": {
     "end_time": "2024-05-28T11:09:24.157706Z",
     "start_time": "2024-05-28T11:09:19.730443Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1 - Fundamentals"
   ],
   "metadata": {
    "id": "kGKikYgZ9W1Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.1\n",
    "During the lecture, you have become familiar with spatial correlation. As a reminder, the formula of the spatial correlation is as follows:\n",
    "\n",
    "$$I'(x,y) = \\sum_{s=-a}^a{\\sum_{t=-b}^b} K(s,t) \\times I(x+s, y+t) $$\n",
    "\n",
    "where\n",
    "- *K* is the Kernel of size $m\\times n$, with $m=2a+1$ and $n=2b+1$ and $a,b > 0$\n",
    "- $x$ and $y$ depict the pixels that are aligned with the center of the kernel\n",
    "- *I* is the input image\n",
    "- *I'* is the output\n",
    "\n",
    "\n",
    "Implement this function in the following code cell without using any package or framework. For now, do not pad the input and assume that we have set ``stride=1``"
   ],
   "metadata": {
    "id": "VRxILORoK8-4"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T12:44:17.575681Z",
     "start_time": "2024-05-28T12:44:17.542883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_correlation(input : np.ndarray, kernel : np.ndarray) -> np.ndarray:\n",
    "  a = (kernel.shape[0]-1)/2\n",
    "  b = (kernel.shape[1]-1)/2\n",
    "  # Der output soll genau inputsize-kernelsize + 1 groß sein\n",
    "  output = np.zeros((input.shape[0]-kernel.shape[0]+1, input.shape[1]-kernel.shape[1]+1))\n",
    "  # Iteriere über jede column\n",
    "  for idxLine, line in enumerate(input):\n",
    "    #Spring raus, wenn man in der ersten oder letzten column ist (da kein padding angegeben wurde)\n",
    "    if idxLine == 0 or idxLine == input.shape[0]-1: continue\n",
    "    # Iteriere über jeden Wert der column\n",
    "    for idxPixel, pixel in enumerate(line):\n",
    "          #Spring raus, wenn man in dem ersten oder letzten Wert einer column ist (da kein padding angegeben wurde)\n",
    "      if idxPixel == 0 or idxPixel == input.shape[1]-1: continue\n",
    "      # Berechne für den jeweiligen Wert den Output Wert, indem die Umgebung mit dem Kernel multipliziert wird und die Werte aufsummiert werden\n",
    "      for idxI, i in enumerate(np.arange(-a,a+1, dtype=int)):\n",
    "        for idxJ, j in enumerate(np.arange(-b,b+1, dtype=int)):\n",
    "          output[idxLine-1, idxPixel-1] += input[idxLine, idxPixel]*kernel[idxI, idxJ]\n",
    "  return output\n",
    "\n",
    "\n",
    "image = np.arange(100).reshape((20,5))\n",
    "kernel = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "\n",
    "output = cross_correlation(image, kernel)\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30.  35.  40.]\n",
      " [ 55.  60.  65.]\n",
      " [ 80.  85.  90.]\n",
      " [105. 110. 115.]\n",
      " [130. 135. 140.]\n",
      " [155. 160. 165.]\n",
      " [180. 185. 190.]\n",
      " [205. 210. 215.]\n",
      " [230. 235. 240.]\n",
      " [255. 260. 265.]\n",
      " [280. 285. 290.]\n",
      " [305. 310. 315.]\n",
      " [330. 335. 340.]\n",
      " [355. 360. 365.]\n",
      " [380. 385. 390.]\n",
      " [405. 410. 415.]\n",
      " [430. 435. 440.]\n",
      " [455. 460. 465.]]\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T12:24:21.237703Z",
     "start_time": "2024-05-28T12:24:21.166773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = np.arange(100).reshape((5,20))\n",
    "kernel = np.array([[1,0,1],[0,1,0],[1,0,1]])\n",
    "\n",
    "output = cross_correlation(image, kernel)\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [20 21 22 23]\n",
      " [40 41 42 43]\n",
      " [60 61 62 63]]\n",
      "(5, 20)\n",
      "[-1  0  1]\n",
      "[-1  0  1]\n",
      "3\n",
      "18\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pixel = 20\n",
      "shape1 - 2 18\n",
      "pixel = 21\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 1\n",
      "computedValueForOutput: 105\n",
      "pixel = 22\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 2\n",
      "computedValueForOutput: 110\n",
      "pixel = 23\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 3\n",
      "computedValueForOutput: 115\n",
      "pixel = 24\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 4\n",
      "computedValueForOutput: 120\n",
      "pixel = 25\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 5\n",
      "computedValueForOutput: 125\n",
      "pixel = 26\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 6\n",
      "computedValueForOutput: 130\n",
      "pixel = 27\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 7\n",
      "computedValueForOutput: 135\n",
      "pixel = 28\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 8\n",
      "computedValueForOutput: 140\n",
      "pixel = 29\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 9\n",
      "computedValueForOutput: 145\n",
      "pixel = 30\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 10\n",
      "computedValueForOutput: 150\n",
      "pixel = 31\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 11\n",
      "computedValueForOutput: 155\n",
      "pixel = 32\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 12\n",
      "computedValueForOutput: 160\n",
      "pixel = 33\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 13\n",
      "computedValueForOutput: 165\n",
      "pixel = 34\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 14\n",
      "computedValueForOutput: 170\n",
      "pixel = 35\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 15\n",
      "computedValueForOutput: 175\n",
      "pixel = 36\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 16\n",
      "computedValueForOutput: 180\n",
      "pixel = 37\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 17\n",
      "computedValueForOutput: 185\n",
      "pixel = 38\n",
      "shape1 - 2 18\n",
      "idx: 1\n",
      "idxPixel: 18\n",
      "computedValueForOutput: 190\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of bounds for axis 1 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[99], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m image \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m100\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m20\u001B[39m))\n\u001B[0;32m      2\u001B[0m kernel \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([[\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m],[\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m0\u001B[39m],[\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m]])\n\u001B[1;32m----> 4\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcross_correlation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "Cell \u001B[1;32mIn[98], line 28\u001B[0m, in \u001B[0;36mcross_correlation\u001B[1;34m(input, kernel)\u001B[0m\n\u001B[0;32m     26\u001B[0m         computedValueForOutput \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m[idx, idxPixel]\u001B[38;5;241m*\u001B[39mkernel[idxI, idxJ]\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomputedValueForOutput: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcomputedValueForOutput\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 28\u001B[0m     \u001B[43moutput\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midxPixel\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m computedValueForOutput\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(output)\n",
      "\u001B[1;31mIndexError\u001B[0m: index 18 is out of bounds for axis 1 with size 18"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.2\n",
    "\n",
    "Now, we also want to include **padding** of the input. Adjust your implementation of ``cross_correlation()``, such that it pads the input (zero-padding) in a way, that you can center the kernel to each original pixel of the input."
   ],
   "metadata": {
    "id": "sfbgcHWPd5JL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def cross_correlation_full(input : np.ndarray, kernel : np.ndarray) -> np.ndarray:\n",
    "  # Your code goes here\n",
    "  raise NotImplementedError"
   ],
   "metadata": {
    "id": "f6P6HTcZd4WP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.3\n",
    "In the lecture, you also learned about convolution. What are the differences between cross-correlation and convolution?"
   ],
   "metadata": {
    "id": "wA94LTdwqsy8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your answer goes here"
   ],
   "metadata": {
    "id": "xpnaxLdKrYfh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.4\n",
    "\n",
    "Provide the implementation of convolution in the following. The implementation should use **same padding** with zeros."
   ],
   "metadata": {
    "id": "nS7WvLqLrapy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def convolution2d(input : np.ndarray, kernel : np.ndarray) -> np.ndarray:\n",
    "  # Your code goes here\n",
    "  raise NotImplementedError"
   ],
   "metadata": {
    "id": "lCjof2w6rmVJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, convolve the flower image with a mean filter. Display both, the input image and the output of the convolution.\n",
    "\n",
    "What happened? Explain.\n",
    "\n",
    "Hint: A larger kernel may help to better identify the differences when the images are displayed small."
   ],
   "metadata": {
    "id": "0czyvfBSakGB"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T21:31:13.953466Z",
     "start_time": "2024-05-15T21:31:11.292259Z"
    }
   },
   "cell_type": "code",
   "source": "flowers = cv2.cvtColor(url2img(\"https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/crush.png?raw=true\"),cv2.COLOR_BGR2GRAY)",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Your code goes here\n",
    "kernel = ..."
   ],
   "metadata": {
    "id": "bc6l23-xaoPz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your answer goes here."
   ],
   "metadata": {
    "id": "YQjvbWjgl4MW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.5\n",
    "\n",
    "In a further step, we also want to convolve an RGB image channel wise. In detail, the input is an RGB image with 3 channels and we have a kernel of shape $(n,n)$. The output should have the same shape as the input, therefore, we want to apply zero-padding to the input. Afterwards, display the image.\n"
   ],
   "metadata": {
    "id": "M5JCCBXz-43J"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T21:31:17.234290Z",
     "start_time": "2024-05-15T21:31:15.128107Z"
    }
   },
   "cell_type": "code",
   "source": "flowers_rgb = cv2.cvtColor(url2img(\"https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/crush.png?raw=true\"),cv2.COLOR_BGR2RGB)",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def convolution_channelwise(input : np.ndarray, kernel : np.ndarray) -> np.ndarray:\n",
    "  # Your code goes here\n",
    "  raise NotImplementedError"
   ],
   "metadata": {
    "id": "wF_vZzSL-8SN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.6\n",
    "\n",
    "The following kernel is separable. Find $w_1$ and $w_2$, such that $w = w_1 \\times w_2$\n",
    "$$w = \\frac{1}{24}\\left[\\begin{array}{rrr}\n",
    "1 & 24 & 3 \\\\\n",
    "\\frac{4}{3} & 32 & 4 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{array}\\right]$$"
   ],
   "metadata": {
    "id": "1ELxwgOnx9kH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.7\n",
    "\n",
    "Show, that the Gaussian kernel $G(s,t)$ from the lecture is separable.\n",
    "$$ G(s,t) =  \\frac{1}{(2\\pi\\sigma^2)} * e^-(\\frac{x^2 + y^2}{2\\sigma^2}) $$"
   ],
   "metadata": {
    "id": "coPQb3sHpi1M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2 - Low-pass Filter"
   ],
   "metadata": {
    "id": "ZR7ZuNvapJCv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 2.1\n",
    "- On a higher level: What are low-pass filters and what purpose do they serve?\n",
    "- How do they work?"
   ],
   "metadata": {
    "id": "HrtZazY2l60a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your answer goes here."
   ],
   "metadata": {
    "id": "oW5Wqm8QAHFL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 2.2\n",
    "\n",
    "The image of Crush the turtle got a little bit corrupted and noise was added. We want to fix this by smoothing the image with the help of low-pass filters.\n",
    "\n",
    "But first, we want to see how noisy the image actually got. For this, we can use the Mean-squared error and compare the original image of crush to the noisy image.\n",
    "\n",
    "The mean-squared error is defined as:\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^n (I_i - \\hat{I}_i)^2$, where\n",
    "- $n$ is the total number of pixels\n",
    "- $I(i)$ and $\\hat{I}(i)$ are the intensity values of pixel $i$ in the original and output images.\n",
    "\n",
    "Implement the mean-squared error for grayscale images AND rgb images."
   ],
   "metadata": {
    "id": "WlYoxukVSNwE"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:12:37.883097Z",
     "start_time": "2024-05-02T15:12:36.399620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "crush_original = cv2.cvtColor(url2img(\"https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/crush.png?raw=true\"), cv2.COLOR_BGR2GRAY)\n",
    "crush_noisy = cv2.cvtColor(url2img(\"https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/crush_noisy.png?raw=true\"), cv2.COLOR_BGR2GRAY)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16,8))\n",
    "ax0.imshow(crush_original, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax0.set_title(\"Crush\")\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(crush_noisy, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax1.set_title(\"Crush + Noise\")\n",
    "ax1.axis(\"off\")\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "id": "qLddZgXos-Vs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here\n",
    "def mse(input1 : np.ndarray, input2 : np.ndarray) -> float:\n",
    "  raise NotImplementedError"
   ],
   "metadata": {
    "id": "QsGHoU2nXfba"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 2.3\n",
    "\n",
    "A simple approach would be the mean filter. Implement the mean filter, compute the mse of the output and the original image and display your output image. For this, use your implementation from exercise 1.\n",
    "\n",
    "(In case you were not able to provide a solution for exercise 1.2 or exercise 1.5, you are allowed to use the `convolve2d()` or `correlate2d()` implementation from `scipy`. This also holds for all exercises below that depend of these.)"
   ],
   "metadata": {
    "id": "eoHozFZwXj9l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here"
   ],
   "metadata": {
    "id": "5n2lSxVwXjiw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 2.4\n",
    "\n",
    "Another possible approach would be the Gaussian kernel. However, we need to define the kernel before.\n",
    "\n",
    "Implement a Gaussian kernel generator that outputs Gaussian filters. The function takes two inputs as parameters:\n",
    "- `kernel_size` defines the size of the quadratic, odd-shaped kernel, e.g., $5\\times 5$.\n",
    "- `sigma` depicts the standard deviation of the Gaussian distribution"
   ],
   "metadata": {
    "id": "B9cDr0rOZZBI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here\n",
    "def gaussian_kernel_generator(kernel_size : int, sigma : float) -> np.ndarray:\n",
    "  raise NotImplementedError"
   ],
   "metadata": {
    "id": "Ig_4UjXqZb2f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to find a good gaussian kernel, provide 5 different combinations of parameters. By doing so, smooth the image with the respective gaussian kernels and compute the mse between the outputs and the original image. Lastly, display the best result."
   ],
   "metadata": {
    "id": "b7V9TbViZr32"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here"
   ],
   "metadata": {
    "id": "oIUTyG7qaJSJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your answer goes here"
   ],
   "metadata": {
    "id": "SoalCC1VaLN3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the following cell to display the original image, the output of the mean-filter and the output of the gaussian filter."
   ],
   "metadata": {
    "id": "7HFcbuJ8bomi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "crush_mean = ...\n",
    "crush_gaussian = ...\n",
    "\n",
    "fig, (ax0, ax1, ax2, ax3) = plt.pyplot.subplots(ncols=4, figsize=(20,10))\n",
    "ax0.imshow(crush_original, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax0.set_title(\"Original\")\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(crush_noisy, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax1.set_title(\"Noise\")\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(crush_mean, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax2.set_title(\"Mean Filter\")\n",
    "ax2.axis(\"off\")\n",
    "ax3.imshow(crush_gaussian, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax3.set_title(\"Gaussian Filter\")\n",
    "ax3.axis(\"off\")\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "id": "sJr2o2vJby6P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3 - Sobel Filter\n",
    "\n",
    "The [Sobel Filter](https://en.wikipedia.org/wiki/Sobel_operator) is a high-pass filter, which can be used for edge detection, but also for image sharpening.\n",
    "\n",
    "In the following, we first want to use the filter to detect the edges of our RGB butterfly image. Then, we can use the the output in order to sharpen a blurr version of the flower image."
   ],
   "metadata": {
    "id": "rOn9bTaApMTW"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "butterfly = cv2.cvtColor(url2img(\"https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/butterfly.png?raw=true\"),cv2.COLOR_BGR2RGB)\n",
    "butterfly_blurred = cv2.cvtColor(url2img(\"https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/butterfly_blurred.png?raw=true\"),cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "plt.imshow(butterfly_blurred, vmin=0, vmax=255)"
   ],
   "metadata": {
    "id": "Sg7Mj6duuhVl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3.1\n",
    "Answer the following questions:\n",
    "- What are high-pass filters and what are they used for?\n",
    "- How are they different from low-pass filter?"
   ],
   "metadata": {
    "id": "pj_ey7mmmLPC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your answer goes here"
   ],
   "metadata": {
    "id": "Ohxao9e3qsW5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "Implement the Sobel filter while using the implementations of exercise 1. Apply the filter channel-wise and display your results."
   ],
   "metadata": {
    "id": "KAfVdFxznPfp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here"
   ],
   "metadata": {
    "id": "0yjLKealqcYX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3.3\n",
    "\n",
    "Use the detected edges to sharpen the blurred image and display the blurred butterfly image and the sharpened image."
   ],
   "metadata": {
    "id": "fXZj9i4gurIg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here"
   ],
   "metadata": {
    "id": "yg03FGWeutfO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "Another high-pass filter is the Laplacian filter. This filter is also used as a edge detector.\n",
    "\n",
    "Apply the Laplacian filter to the original butterfly image and display the edges of all three channels separately."
   ],
   "metadata": {
    "id": "D6ni0YzS9b13"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here"
   ],
   "metadata": {
    "id": "qC3A1nWt9bU1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 4 - Canny"
   ],
   "metadata": {
    "id": "y1ZqQyRE9Pm7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4.1\n",
    "\n",
    "Besides the aforementioned high-pass filters, there are also other types of apporaches, that are used for edge detection. For example, the [Canny edge detector](https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html). Familiarize yourself with this algorithm and apply it to our flowers image.\n",
    "\n",
    "Hint: You do not have to implement Canny edge detector on your own, you are allowed to use any library."
   ],
   "metadata": {
    "id": "QhTsg5Qz9VoQ"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:13:02.170776Z",
     "start_time": "2024-05-02T15:13:01.504700Z"
    }
   },
   "cell_type": "code",
   "source": "flowers = cv2.cvtColor(url2img(\"https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/flowers.jpg?raw=true\"),cv2.COLOR_BGR2GRAY)\n",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Your code goes here\n",
    "flowers_canny = ...\n"
   ],
   "metadata": {
    "id": "5xRZQov59YjD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4.2\n",
    "\n",
    "Use the Sobel filter again for the flowers image and compare the output to the Canny output.\n",
    "\n",
    "What differences can you see?\n",
    "Which detection approach would you prefer in this case and why?\n",
    "\n",
    "Hint: For this task you can just use the grayscale image."
   ],
   "metadata": {
    "id": "8pyKL9ao9nez"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here\n",
    "flowers_sobel = ..."
   ],
   "metadata": {
    "id": "QxZktjzf9m4f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(16,4))\n",
    "\n",
    "ax0.imshow(flowers, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax0.set_title(\"Input image\")\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(flowers_canny, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax1.set_title(\"Canny\")\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(flowers_sobel, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax2.set_title(\"Sobel Filter\")\n",
    "ax2.axis(\"off\")\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "id": "_v-3nA7DrdOZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your answer goes here."
   ],
   "metadata": {
    "id": "TvUD7tn4rFg-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second Part - Filtering in the Frequency Domain\n",
    "\n"
   ],
   "metadata": {
    "id": "tyoM1_yiYILG"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:13:13.480233Z",
     "start_time": "2024-05-02T15:13:13.475364Z"
    }
   },
   "cell_type": "code",
   "source": "url = 'https://www.shutterstock.com/image-photo/view-city-koeln-cologne-germany-600nw-107359316.jpg'",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "img = url2img(url)\n",
    "grayscale_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(grayscale_img, cmap='gray')\n",
    "plt.axis(False)"
   ],
   "metadata": {
    "id": "RALiYUX2Ynt6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 5 - 2D Discrete Fourier Transform\n",
    "\n",
    "\n",
    "The 2D DFT is defined as\n",
    "\n",
    "$$\n",
    "F(u,v) =\\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y) e^{-j2\\pi(v\\frac{y}{N}+u\\frac{x}{M} )},\n",
    "$$\n",
    "\n",
    "which we can manipulate:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "F(u,v)  & = \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y) e^{-j2\\pi(v\\frac{y}{N}+u\\frac{x}{M} )} \\\\\n",
    " & = \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y) e^{-j2\\pi v \\frac{y}{N}} e^{-j2\\pi u \\frac{x}{M}} \\\\\n",
    " & = \\sum_{x=0}^{M-1} \\left[\\sum_{y=0}^{N-1} f(x,y) e^{-j2\\pi v \\frac{y}{N}} \\right] e^{-j2\\pi u \\frac{x}{M}},\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the term in square brackets is the 1D DFT of the x-th column of the image.  \n",
    " This means that we can replace each column of an image by its 1D DFT, and then compute the row-wise 1D DFT.\n",
    "\n",
    "- Implement a function to compute the **centred** 2D DFT of a grayscale image given as a numpy matrix.\n",
    "\n",
    "- Implement the inverse transform.\n",
    "\n",
    "You can use a library 1D transform, for example `numpy.fft.fft`. **DO NOT** use a library 2D DFT transform for this exercise.\n",
    "\n",
    "Exemplify your function by visualising the (centred) _spectrum_ and _phase_ of some images.\n",
    "\n",
    "For a better visualization, remember to use a log transform on the spectra\n"
   ],
   "metadata": {
    "id": "XPq4lI1-YhVf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here:\n",
    "\n",
    "# Implement 2D Discrite Fourier Transform and its Inverse\n",
    "\n",
    "# Plot images and their centred, log-transformed spectra"
   ],
   "metadata": {
    "id": "N2wwz9lfYsoN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 6 - Spectrum manipulation\n",
    "The following images have been corrupted by periodic noise. Fix their DFT spectra and show the results. You can use any image editor software (e.g. MS Paint).\n",
    "\n",
    "Show:\n",
    "- The DFT spectrum of the corrupted image\n",
    "- The \"fixed\" DFT spectrum.\n",
    "- The restored image.\n",
    "\n",
    "Hint: remember to take the phase into account when doing the IDFT.\n",
    "\n",
    "![Corrupted image](https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/corrupted_img.png?raw=true)\n",
    "\n",
    "![Corrupted image](https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/moon.jpg?raw=true)"
   ],
   "metadata": {
    "id": "87SHarjMYtiy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here:\n",
    "\n",
    "# Plot the spectra of the images above\n",
    "\n",
    "# Edit the spectra, with your favorite tool, to remove the periodic noise\n",
    "\n",
    "# Upload the new spectra to the notebook, and visualize it\n",
    "\n",
    "# Transform the spectra to space domain, and plot the restored images"
   ],
   "metadata": {
    "id": "mXieUm-YYusJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714638030574,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Florian Bürger",
      "userId": "11969544149534768288"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 7 - Filters in Frequency Space\n",
    "Implement the following filters in the frequency domain:\n",
    "- Ideal LPF\n",
    "- Gaussian LPF\n",
    "- Butterworth LPF\n",
    "- Ideal HPF\n",
    "- Gaussian HPF\n",
    "- Butterworth HPF\n",
    "\n",
    "The functions should:\n",
    "- take a grayscale image as input\n",
    "  - you can extend it to RGB\n",
    "- convert them to the frequency domain\n",
    "- compute the filter kernels in the frequency domain\n",
    "- apply the filter in frequency domain\n",
    "- compute the inverse transform\n",
    "- return the inverse transform as a grayscale image.\n",
    "\n",
    "Visualise the results of applying these filters on some images"
   ],
   "metadata": {
    "id": "0xJ-pOJdZTNq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here:\n",
    "\n",
    "# Implement the above mentioned filters\n",
    "\n",
    "# Plot one image, and the filters outputs\n",
    "\n",
    "# example prototype\n",
    "def apply_gaussian_filter(img, d0):\n",
    "  # ...\n",
    "  return"
   ],
   "metadata": {
    "id": "4d0SBjicZaGd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 8 - Deconvolution\n",
    "You are a Remote Sensing Engineer at DLR, and your team has recently launched a new space telescope. Unfortunately, due to undetected mirror faults, the images it acquires are not worth the billion euros of tax-payer money that were spent on it.  \n",
    "\n",
    "![galaxy shot](https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/telescope_broken.png?raw=true)\n",
    "\n",
    "Luckily, since you've taken Prof. Bozek's course, you can spare the costs of a repair mission by fixing the images here in Earth.\n",
    "\n",
    "This is a shot of a distant lone star taken by the telescope. Being such a small object, a working shot of the star should look like a single impulse, and is a good approximation of the [Point Spread Function](https://en.wikipedia.org/wiki/Point_spread_function) of the damaged telescope.\n"
   ],
   "metadata": {
    "id": "znSjXqA6ZdAZ"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:13:30.282758Z",
     "start_time": "2024-05-02T15:13:30.266614Z"
    }
   },
   "cell_type": "code",
   "source": "url = 'https://github.com/bozeklab/ComputerVision_SS24/blob/main/resources/assignment_2/star_broken_16bits.png?raw=true'",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "img = url2img(url)/ 65535 # this image has 16 bits of depth\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis(False)"
   ],
   "metadata": {
    "id": "-1zhjfzlZiWt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the star shot to fix the galaxy image, and display it. **Implement** the naive deconvolution code. Feel fry to try more advanced deconvolution algorithms."
   ],
   "metadata": {
    "id": "WHFE2YbQZqg6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code goes here\n",
    "\n",
    "# Implement naive deconvolution\n",
    "\n",
    "# Restore the galaxy image using deconvolution\n",
    "\n",
    "# Show the restored image"
   ],
   "metadata": {
    "id": "xsxs8rmjZrHs"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
